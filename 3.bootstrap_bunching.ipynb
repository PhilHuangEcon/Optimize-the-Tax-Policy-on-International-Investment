{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is the draft of bootstrapping bunching estimation.\n",
    "\n",
    "We calculate the Variance of the Dd and Frac_nb based on the original boostrap process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import copy \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/Users/philhuang/Desktop/FDI_emp/workingdata/'\n",
    "output_folder = '/Users/philhuang/Desktop/FDI_emp/tempdata/bunching_bootstrap/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>panel_id</th>\n",
       "      <th>share_fdi</th>\n",
       "      <th>bin_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998</td>\n",
       "      <td>SZ10000019</td>\n",
       "      <td>0.509798</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>SZ10000019</td>\n",
       "      <td>0.509798</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>SZ10000019</td>\n",
       "      <td>0.509798</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>SZ10000019</td>\n",
       "      <td>0.509798</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>SZ10000019</td>\n",
       "      <td>0.509798</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150761</th>\n",
       "      <td>2005</td>\n",
       "      <td>SZ10890785</td>\n",
       "      <td>0.925039</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150762</th>\n",
       "      <td>2000</td>\n",
       "      <td>SZ10890801</td>\n",
       "      <td>0.693488</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150763</th>\n",
       "      <td>2001</td>\n",
       "      <td>SZ10890801</td>\n",
       "      <td>0.699574</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150764</th>\n",
       "      <td>2002</td>\n",
       "      <td>SZ10890801</td>\n",
       "      <td>0.699574</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150765</th>\n",
       "      <td>2003</td>\n",
       "      <td>SZ10890801</td>\n",
       "      <td>0.713131</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150766 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year    panel_id  share_fdi  bin_id\n",
       "0       1998  SZ10000019   0.509798      50\n",
       "1       1999  SZ10000019   0.509798      50\n",
       "2       2000  SZ10000019   0.509798      50\n",
       "3       2001  SZ10000019   0.509798      50\n",
       "4       2002  SZ10000019   0.509798      50\n",
       "...      ...         ...        ...     ...\n",
       "150761  2005  SZ10890785   0.925039      92\n",
       "150762  2000  SZ10890801   0.693488      69\n",
       "150763  2001  SZ10890801   0.699574      69\n",
       "150764  2002  SZ10890801   0.699574      69\n",
       "150765  2003  SZ10890801   0.713131      71\n",
       "\n",
       "[150766 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data.\n",
    "ASIF_df = pd.read_csv(path+'ASIF_panel_bootstrapBucnhing.csv')\n",
    "\n",
    "# Look at the data.\n",
    "ASIF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43063"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique panel_id for the original sample.\n",
    "panel_id_lst = pd.DataFrame(ASIF_df.reset_index().panel_id.unique())\n",
    "orgianl_id_num = len(panel_id_lst)\n",
    "orgianl_id_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to generate a bootstrap sample from the original sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_ASIF(seed_num):\n",
    "    \n",
    "    # Bootstrap panel_id with replacement, and get a list of panel_id of the new sample.\n",
    "    bootstrap_id = panel_id_lst.sample(n=orgianl_id_num, random_state=seed_num, replace=True)\n",
    "    bootstrap_id.rename(columns={0:'panel_id'}, inplace=True)\n",
    "\n",
    "    # Merge the new list of panel_id with the original sample to form a new bootstrapped sample.\n",
    "    rst = bootstrap_id.merge(ASIF_df, on=['panel_id'], how='left')\n",
    "    \n",
    "    return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to get the distribution of FDI share by bin, and generate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_boots(target_df, random_state):\n",
    "    #### Calculate the vars we need for the whole sample.\n",
    "    # Calculate the number of firms in each bin.\n",
    "    overall_bin_count = np.array(target_df.groupby('bin_id')['panel_id'].count())\n",
    "    overall_bin_count\n",
    "\n",
    "    # Calculate the total of all firms.\n",
    "    total_firms = np.sum(overall_bin_count)\n",
    "\n",
    "    # Calculate the prob of firms falling into each bin.\n",
    "    overall_bin_prob = overall_bin_count/total_firms\n",
    "\n",
    "    #### Generate test and training data. \n",
    "    # Generate random numbers between [0,1]\n",
    "    np.random.seed(random_state)\n",
    "    target_df['random_num'] = np.random.uniform(0,1, total_firms)\n",
    "    # Generate ID of the train group based on the random_num. Note that we split the data as follows:\n",
    "    # 20% testing and 80% training.\n",
    "    target_df['test_group_id'] = 1\n",
    "    target_df.loc[target_df['random_num']>=0.2, 'test_group_id'] = 2\n",
    "    target_df.loc[target_df['random_num']>=0.4, 'test_group_id'] = 3\n",
    "    target_df.loc[target_df['random_num']>=0.6, 'test_group_id'] = 4\n",
    "    target_df.loc[target_df['random_num']>=0.8, 'test_group_id'] = 5\n",
    "\n",
    "    # Store the results.\n",
    "    train_data = []\n",
    "    test_data  = []\n",
    "\n",
    "    for i in range(1,6):\n",
    "        # Take out each test group.\n",
    "        temp_df     = target_df[target_df['test_group_id']==i]\n",
    "        # Calculate the prob of firms falling into each bin in the test group.\n",
    "        temp_prob_bin = np.array(temp_df.groupby('bin_id')['panel_id'].count()/len(temp_df))\n",
    "        # Store the result to the list.\n",
    "        test_data.append(temp_prob_bin)\n",
    "\n",
    "        # Take out each training group.\n",
    "        temp_df     = target_df[target_df['test_group_id']!=i]\n",
    "        # Calculate the prob of firms falling into each bin in the test group.\n",
    "        temp_prob_bin = np.array(temp_df.groupby('bin_id')['panel_id'].count()/len(temp_df))\n",
    "        # Store the result to the list.\n",
    "        train_data.append(temp_prob_bin)\n",
    "\n",
    "    # Return things in a DataFrame\n",
    "    df_start                  = pd.DataFrame(np.array(list(range(0,100)))/100, columns=['bin'])\n",
    "    df_start['count']         = overall_bin_prob\n",
    "    df_start['count_train_1'] = train_data[0]\n",
    "    df_start['count_train_2'] = train_data[1]\n",
    "    df_start['count_train_3'] = train_data[2]\n",
    "    df_start['count_train_4'] = train_data[3]\n",
    "    df_start['count_train_5'] = train_data[4]\n",
    "    df_start['count_test_1']  = train_data[0]\n",
    "    df_start['count_test_2']  = train_data[1]\n",
    "    df_start['count_test_3']  = train_data[2]\n",
    "    df_start['count_test_4']  = train_data[3]\n",
    "    df_start['count_test_5']  = train_data[4]\n",
    "\n",
    "    return df_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now estimate bunching using the dataset we have at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grp2idx function that we will use later in the find_Dm function\n",
    "# This is equivalent to grp2idx function in Matlab\n",
    "# This convert [0,0,1,1,1,0,0] to [1,1,2,3,4,1,1]\n",
    "def grp2idx(lst):\n",
    "    rst = []\n",
    "    category = 1\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == 0:\n",
    "            rst.append(1)\n",
    "        else:\n",
    "            category += 1\n",
    "            rst.append(category)\n",
    "    return np.array(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function constructs w_matrix, which is the polynomials and fixed effect;\n",
    "# df_dx, which is the first order derivative w.r.t x of w_matrix\n",
    "# db, which is just a vector of 0.\n",
    "\n",
    "def construct_matrix(minD, p, Dp, Dm, X, bins):\n",
    "    # This part adds round number fixed effects to the regression\n",
    "    round_num_lst = np.array(list(range(25, 100, 5)))/100\n",
    "    len_round_num = 3                                            # We only need three round-num dummy here. Including 0.5 fixed effect.\n",
    "    dummy_5=[]\n",
    "    dummy_10=[]\n",
    "    dummy_50=[]\n",
    "    for item in all_bins[all_bins>minD]:\n",
    "        if item in round_num_lst and int(item*100)%10 ==5:\n",
    "            dummy_5.append(1)\n",
    "        else:\n",
    "            dummy_5.append(0)\n",
    "    for item in all_bins[all_bins>minD]:\n",
    "        if item in round_num_lst and int(item*100)%10 ==0:\n",
    "            dummy_10.append(1)\n",
    "        else:\n",
    "            dummy_10.append(0)\n",
    "    for item in all_bins[all_bins>minD]:\n",
    "        if item == 0.5:\n",
    "            dummy_50.append(1)\n",
    "        else:\n",
    "            dummy_50.append(0)\n",
    "            \n",
    "    round_num_FE = pd.DataFrame({'dummy_5':dummy_5, 'dummy_10':dummy_10, 'dummy_50':dummy_50})\n",
    "    \n",
    "    # This part adds bunching fixed effect to the regression (the excluded region). \n",
    "    group = bins[bins>minD] * ((bins[bins>minD] > Dm) & (bins[bins>minD] < Dp))\n",
    "    group = grp2idx(group)\n",
    "    group_df = pd.DataFrame(data = group, columns=['group'])\n",
    "    FE = pd.get_dummies(group_df.group)\n",
    "    w = pd.concat([X, round_num_FE, FE.iloc[:,1:]], axis=1)      # Here we get p-polynomials of the edges and fixed effect\n",
    "    w.columns=list(range(len(w.columns)))                        # between Dm and Dp\n",
    "    w_matrix = np.array(w)\n",
    "    \n",
    "    # This part calculates the derivative matrix.\n",
    "    const = ((bins[bins>minD] > Dm) & (bins[bins>minD] < notch)).astype(int)\n",
    "    dx=[0*const, const]\n",
    "    for j in range(2,p+1):\n",
    "        dx.append(np.array(j*const*bins[bins>minD]**(j-1)))      # gradient of the polynomials\n",
    "    df_dx = pd.DataFrame(data=np.array(dx).T)\n",
    "    df_dx = pd.concat([df_dx, 0*round_num_FE, 0*FE.iloc[:,1:]], axis=1)\n",
    "    df_dx.columns=list(range(len(df_dx.columns)))\n",
    "    df_dx = np.array(df_dx)\n",
    "    \n",
    "    db = np.zeros(len(all_bins[all_bins>minD]))                                 \n",
    "    \n",
    "    return w_matrix, df_dx, db, len_round_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function solves for the optimal b under the constraint\n",
    "\n",
    "def get_beta(w_matrix, C_minD_end, df_dx):\n",
    "        b0 = np.linalg.lstsq(w_matrix, C_minD_end, rcond=None)[0]    # b0 is the unconstrained beta. Now the have 76 equations\n",
    "                                                                     # but 28 parameters, we have to use ols to estimate b\n",
    "        def objective(b):                                            # Calculate objective f\n",
    "            resid = C_minD_end-w_matrix.dot(b)\n",
    "            f = (resid.T).dot(resid)\n",
    "            return f\n",
    "\n",
    "        def constraint1(b):                                          # note in Python scipy.minimize, the constraint should be f(x)>=0,\n",
    "            g = -df_dx.dot(b)                                        # so we set g to be the opposite of the F.O.C\n",
    "            return g\n",
    "\n",
    "        cons1 = {'type':'ineq', 'fun': constraint1}\n",
    "        #sol = minimize(objective, b0, method='SLSQP', constraints=cons1)\n",
    "        sol = minimize(objective, b0, method='SLSQP')\n",
    "\n",
    "        b = sol.x\n",
    "        return b \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the lower bound of bunching.\n",
    "def find_Dm(minD, p, Dp, X, C, bins, notch, nReps):\n",
    "    results = []\n",
    "    C_minD_end = C[bins>minD]\n",
    "    d = bins[1] - bins[0]\n",
    "    lst_Dm = bins[(bins>(minD*1.0+4*d)) & (bins < (notch*1.0-8*d))] # all possible Dm\n",
    "\n",
    "    for i in lst_Dm:\n",
    "        Dm = i\n",
    "        ## Here the w matrix: 25 with FE, but not round number effect \n",
    "        w_matrix, df_dx, db, len_round_num = construct_matrix(minD, p, Dp, Dm, X, bins)\n",
    "        b = get_beta(w_matrix, C_minD_end, df_dx)\n",
    "\n",
    "        ## Here the w_matrix: 25 with FE, but not round number effect \n",
    "        criterion_Dm_Dp= ((bins>Dm) & (bins<Dp))[-len(w_matrix):]     \n",
    "        results.append([Dm, \n",
    "                        np.sum(w_matrix[criterion_Dm_Dp][:,:p+len_round_num+1].dot(b[:p+len_round_num+1])         ## What we get here is negative FE: polynomial + Rnum effect - the observed data\n",
    "                               - w_matrix[criterion_Dm_Dp].dot(b)),                                               ## This part has FE, which is a perfect fit of the observed data \n",
    "                        np.sum(w_matrix[criterion_Dm_Dp][:,:p+len_round_num+1].dot(b[:p+len_round_num+1])),       ## counterfactual C \n",
    "                        np.sum(w_matrix[criterion_Dm_Dp][:, p+len_round_num+1:].dot(b[p+len_round_num+1:]))])     ## FE(excluded region)\n",
    "    ## Results\n",
    "    results_df = pd.DataFrame(np.array(results))\n",
    "    # There might be multiple candidates, we choose the smallest one. The Dm satisfy: the effct of excluded\n",
    "    # region FE is the smallest.\n",
    "    Dm   = results_df[abs(results_df[1]) == abs(results_df[1]).min()][0].min() \n",
    "    diff = results_df[results_df[0]==Dm][1].min()                               ## negative FE\n",
    "    DD   = results_df[results_df[0]==Dm][2].min()\n",
    "\n",
    "    ## With the optimal Dm, we now get main estimation. Note that here we only use the w matrix for estimation \n",
    "    w_matrix, df_dx, db, len_round_num = construct_matrix(minD, p, Dp, Dm, X, bins)\n",
    "    b = get_beta(w_matrix, C_minD_end, df_dx)\n",
    "    resid = w_matrix.dot(b) - C_minD_end\n",
    "\n",
    "    ## Bootstrap b and DD\n",
    "    id_matrix = np.ceil(np.random.rand(len(resid), nReps)*len(resid))     # get random numbers as the bstrap id. There can be repeated numbers in \n",
    "                                                                          # one column,which means we draw the residual with replacement\n",
    "    diff_b =[]\n",
    "    for i in range(nReps):\n",
    "        tempC = C_minD_end + resid[(id_matrix[:,i]-1).astype(int)]          \n",
    "        btrap_b = get_beta(w_matrix, tempC, df_dx)\n",
    "\n",
    "        # Note now we have a new Dm (the optimal Dm), we need to specify a new criterion_Dm_Dp\n",
    "        # Now we use the w matrix for prediction\n",
    "        criterion_Dm_Dp= ((bins>Dm) & (bins<Dp))[-len(w_matrix):]\n",
    "        diff_b.append(np.sum(w_matrix[criterion_Dm_Dp][:, :p+len_round_num+1].dot(btrap_b[:p+len_round_num+1]) \n",
    "                             - w_matrix[criterion_Dm_Dp][:, p+len_round_num+1:].dot(btrap_b[p+len_round_num+1:])))\n",
    "\n",
    "    pval = 2*(1- norm.cdf(abs(diff/np.std(diff_b))))\n",
    "    \n",
    "    return Dm, pval, DD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate SSE, which helps to find the best estimator which minimizes SSE.\n",
    "def SSE_K(minD, p, Dp, C, df_C, bins, notch, nReps):\n",
    "    # Generate polynomials\n",
    "    x = bins[bins>minD]\n",
    "    xs = []\n",
    "    for i in range(p+1):\n",
    "        xs.append(x**i)\n",
    "    xs=np.array(xs).T\n",
    "    df_x = pd.DataFrame(data=xs)\n",
    "\n",
    "    # Find Dm \n",
    "    Dm,pval,DD = find_Dm(minD, p, Dp, df_x, C, bins, notch, nReps) \n",
    "    w_matrix, df_dx, db, len_round_num= construct_matrix(minD, p, Dp, Dm, df_x, bins)\n",
    "    \n",
    "    # Compute SSE for K-Fold CV\n",
    "    SSE = 0 \n",
    "    for k in range(1,6):\n",
    "        C_train = np.array(df_C.iloc[:,k+1])[bins>minD]\n",
    "        C_test  = np.array(df_C.iloc[:,k+6])[bins>minD]\n",
    "        b_Kfold = get_beta(w_matrix, C_train, df_dx)\n",
    "        SSE +=  sum((w_matrix.dot(b_Kfold)- C_test)**2) \n",
    "    SSE = 1000*SSE/len(bins[bins>minD])\n",
    "    return Dm, SSE, pval, DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate p,Dp, minD, Dm, SSE, pval, DD\n",
    "def get_pDpminDDm(work_df, notch, nReps):\n",
    "\n",
    "    # Fetch the information of bin-cut threshold and the number of firms in each bin.\n",
    "    #all_bins = np.around(np.array(work_df['bin']), 3)\n",
    "    #C_all = np.array(work_df['count'])\n",
    "\n",
    "    # Generate Combinations of (p,Dp,minD) \n",
    "    param = []\n",
    "    lst_Dp = all_bins[(all_bins > notch*1.05) & (all_bins <=0.295)]\n",
    "    #lst_Dp = all_bins[(all_bins > notch*1.15) & (all_bins <0.04)]\n",
    "    lst_minD = all_bins[(all_bins > all_bins[0]) & (all_bins <=0.02)]\n",
    "    #lst_minD = all_bins[(all_bins > all_bins[0]) & (all_bins <=0.003)]\n",
    "    lst_p = [3,4]\n",
    "\n",
    "    for p in lst_p:\n",
    "        for Dp in lst_Dp:\n",
    "            for minD in lst_minD:\n",
    "                param.append([p,Dp,minD])\n",
    "\n",
    "    # Store things in a dataFrame.\n",
    "    df_param = pd.DataFrame(data =param, columns=['p', 'Dp', 'minD'])\n",
    "\n",
    "    # Create a list to store all the SSE of different combinations of p, Dp and minD.\n",
    "    SSE_out = []\n",
    "    for p in lst_p:\n",
    "        for Dp in lst_Dp:\n",
    "            for minD in lst_minD:\n",
    "                Dm, SSE, pval, DD = SSE_K(minD, p, Dp, C_all, work_df, all_bins, notch, nReps)\n",
    "                SSE_out.append([p, Dp, minD, Dm, SSE, pval, DD])\n",
    "\n",
    "    # Convert it into a dataFrame.\n",
    "    SSE_out_df = pd.DataFrame(np.array(SSE_out), columns=['p', 'Dp', 'minD', 'Dm', 'SSE', 'pval', 'DD'])\n",
    "\n",
    "    ## Choose (p,Dp,minD) that minimizes SSE.\n",
    "    sub_SSE_out  = SSE_out_df[SSE_out_df['pval']>0.1].copy()              # pval should be >0.1\n",
    "    model_choice = sub_SSE_out.iloc[np.argmin(sub_SSE_out['SSE']),:]      # choose the params with the smallest SSE\n",
    "    p            = int(model_choice[0])\n",
    "    Dp           = float(model_choice[1])\n",
    "    minD         = float(model_choice[2])\n",
    "    Dm           = float(model_choice[3])\n",
    "    SSE          = float(model_choice[4])\n",
    "    pval         = float(model_choice[5])\n",
    "    DD           = float(model_choice[6])\n",
    "    \n",
    "    return p,Dp, minD, Dm, SSE, pval, DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Est(minD, p, Dp, Dm, C, bins, notch, nReps):\n",
    "    \n",
    "    # Drop all bins that are to the left of the minD bin.\n",
    "    C_minD_end = C[bins>minD]\n",
    "    \n",
    "    # Generate polynomials\n",
    "    x = bins[bins>minD]\n",
    "    xs = []\n",
    "    for i in range(p+1):\n",
    "        xs.append(x**i)\n",
    "    xs=np.array(xs).T\n",
    "    df_x = pd.DataFrame(data=xs)\n",
    "    \n",
    "    w_matrix, df_dx, db, len_round_num = construct_matrix(minD, p, Dp, Dm, df_x, bins)  # w matrix for estimate \n",
    "    b = get_beta(w_matrix, C_minD_end, df_dx)\n",
    "\n",
    "    resid = w_matrix.dot(b) - C_minD_end\n",
    "    \n",
    "    # Calculate the counterfactuual distribution of firms.\n",
    "    h = w_matrix[:,:p+len_round_num+1].dot(b[:p+len_round_num+1])                       # Polynomial + Rnum effect \n",
    "                                                                                        # w matrix for prediction\n",
    "    # Bootstrap b and DD \n",
    "    # We draw the residuals.\n",
    "    id_matrix = np.ceil(np.random.rand(len(resid), nReps)*len(resid))             \n",
    "    b_B = [] \n",
    "    h_B = [] \n",
    "\n",
    "    for i in range(nReps):\n",
    "        # Add the residual to the actual C (prob of firms falling into each bin in the data we observed.)\n",
    "        tempC = C_minD_end + resid[(id_matrix[:,i]-1).astype(int)] \n",
    "        \n",
    "        # Estimate Beta.\n",
    "        b_temp = get_beta(w_matrix, tempC, df_dx)                                   # w matrix for estimation\n",
    "        \n",
    "        # Calculate the predicted prob of firms falling into each bin.\n",
    "        h_temp = w_matrix[:,:p+len_round_num+1].dot(b_temp[:p+len_round_num+1])     # w matrix for prediction   \n",
    "        b_B.append(b_temp)\n",
    "        h_B.append(h_temp)\n",
    "\n",
    "    b_B_df = pd.DataFrame(np.array(b_B))\n",
    "    h_B_df = pd.DataFrame(np.array(h_B))\n",
    "\n",
    "    b_SE = np.array(b_B_df.std(axis=0))\n",
    "    V = np.array(b_B_df.cov())\n",
    "    \n",
    "    return b, b_SE, V, h, h_B_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6- Compute Stats\n",
    "def BStat(Dm, notch, Dp, C, h, h_B, bins, d):\n",
    "    \n",
    "    # Note that here the dimension does not match \n",
    "    # criterion_Dm_notch = ((bins>=Dm) & (bins<notch))[:len(h)]\n",
    "    # criterion_Dm_Dp    = ((bins>=Dm) & (bins<=Dp))[:len(h)]  \n",
    "    criterion_Dm_notch0 = ((bins>minD) & (bins<notch))[-len(h):]\n",
    "    criterion_Dm_notch1 = ((bins>minD) & (bins<=notch))[-len(h):]\n",
    "    #criterion_Dm_Dp    = ((bins>minD) & (bins<=Dp))[-len(h):] \n",
    "    \n",
    "    # Theoretically, all firms should have bunched to the right. However due to firction cost, some firms still stay to the left \n",
    "    # Here we calculate the ratio of real stayers over conterfactual stayers. The higher the ratio, the higher the frition cost \n",
    "    frac_nb    = np.sum(C[(bins>minD)&(bins<notch)]) / np.sum(h[criterion_Dm_notch0])       \n",
    "    frac_nb_B  = np.sum(C[(bins>minD) & (bins<notch)])/np.sum(h_B.iloc[:,criterion_Dm_notch0].copy(), axis=1)\n",
    "    frac_nb_B  = np.array(frac_nb_B)\n",
    "\n",
    "    b_Bstat    = bins[(bins>minD) & (bins<=notch)]+d                                     # upper bound of bins\n",
    "    c_Bstat    = C[(bins>minD) & (bins<=notch)]/ np.sum(C[(bins>minD) & (bins<=notch)])  # real conditional probability\n",
    "    hh         = h[criterion_Dm_notch1]/np.sum(h[criterion_Dm_notch1])                   # counterfactual conditional probability \n",
    "\n",
    "    hh_B = []\n",
    "    temp1 = h_B.iloc[:,criterion_Dm_notch1].copy()\n",
    "    temp2 = np.sum(h_B.iloc[:,criterion_Dm_notch1].copy(), axis=1)\n",
    "    for i in range(len(temp1)):\n",
    "        hh_B.append((temp1.iloc[i,:]/temp2[i]))\n",
    "    hh_B_df = pd.DataFrame(np.array(hh_B))\n",
    "    \n",
    "    #Dd = b_Bstat.dot(c_Bstat - hh)/(b_Bstat.dot(c_Bstat))                      # distance between y and y_hat \n",
    "    Dd = b_Bstat.dot(c_Bstat - hh)/(b_Bstat.dot(hh))                            # distance between y and y_hat \n",
    "\n",
    "    \n",
    "    Dd_B_temp=[]\n",
    "    for i in range(len(hh_B_df.T.columns)):\n",
    "        Dd_B_temp.append(c_Bstat-hh_B_df.T.iloc[:,i])\n",
    "    Dd_B = b_Bstat.dot(np.array(Dd_B_temp).T)/(b_Bstat.dot(c_Bstat))\n",
    "    \n",
    "    return Dd, Dd_B, frac_nb, frac_nb_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:03.110260\n"
     ]
    }
   ],
   "source": [
    "# Specify notch point. \n",
    "notch          = 0.25 \n",
    "nReps          = 2000 \n",
    "d              = 0.01\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Then we generate the test and training set.\n",
    "test_train_df = get_test_train_boots(ASIF_df, 12345)\n",
    "\n",
    "# Fetch the information of bin-cut threshold and the number of firms in each bin.\n",
    "all_bins = np.around(np.array(test_train_df['bin']), 3)\n",
    "C_all    = np.array(test_train_df['count'])\n",
    "\n",
    "# Estimate some basic bunching parameters.\n",
    "p,Dp, minD, Dm, SSE, pval, DD = get_pDpminDDm(test_train_df, notch, 50)\n",
    "\n",
    "# Estimate beta, and also the bootstrapped h.\n",
    "b,b_SE,V,h,h_B  = Model_Est(minD,p,Dp,Dm,C_all,all_bins,notch, nReps)\n",
    "\n",
    "# Get bootstrapped Dd and frac_nb.\n",
    "Dd,Dd_B,frac_nb,frac_nb_B = BStat(Dm, notch, Dp, C_all, h, h_B, all_bins, d)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dd_B</th>\n",
       "      <th>frac_nb_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.220084</td>\n",
       "      <td>0.404314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.210792</td>\n",
       "      <td>0.363581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.241907</td>\n",
       "      <td>0.416094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.215567</td>\n",
       "      <td>0.381341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.202672</td>\n",
       "      <td>0.488823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.180549</td>\n",
       "      <td>0.440618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.234301</td>\n",
       "      <td>0.370485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.220917</td>\n",
       "      <td>0.517284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.197192</td>\n",
       "      <td>0.361469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.211784</td>\n",
       "      <td>0.411186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dd_B  frac_nb_B\n",
       "0     0.220084   0.404314\n",
       "1     0.210792   0.363581\n",
       "2     0.241907   0.416094\n",
       "3     0.215567   0.381341\n",
       "4     0.202672   0.488823\n",
       "...        ...        ...\n",
       "1995  0.180549   0.440618\n",
       "1996  0.234301   0.370485\n",
       "1997  0.220917   0.517284\n",
       "1998  0.197192   0.361469\n",
       "1999  0.211784   0.411186\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunching_momnet_BSrst = pd.DataFrame(data={'Dd_B':Dd_B, 'frac_nb_B':frac_nb_B})\n",
    "bunching_momnet_BSrst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dd_B</th>\n",
       "      <th>frac_nb_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dd_B</th>\n",
       "      <td>0.001202</td>\n",
       "      <td>-0.001867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frac_nb_B</th>\n",
       "      <td>-0.001867</td>\n",
       "      <td>0.007888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dd_B  frac_nb_B\n",
       "Dd_B       0.001202  -0.001867\n",
       "frac_nb_B -0.001867   0.007888"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunching_momnet_BSrst.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunching_momnet_BSrst.to_csv(output_folder+'bootstrap_bunching12June2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " 0.28,\n",
       " 0.01,\n",
       " 0.06,\n",
       " 0.09182893633940276,\n",
       " 0.9843804207240385,\n",
       " 0.2546654364683665)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p,Dp, minD, Dm, SSE, pval, DD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.208185626943552, 0.460948376140406)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Dd_B.mean(), frac_nb_B.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
